{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tuto PyTorch + Digit Recognition (PyTorch).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIKevin/Deep-Learning/blob/master/Tuto_PyTorch_+_Digit_Recognition_(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rMChH3clPg_O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Computational graphs\n",
        "The first thing to understand about any deep learning library is the idea of a computational graph. A computational graph is a set of calculations, which are called nodes, and these nodes are connected in a directional ordering of computation. In other words, some nodes are dependent on other nodes for their input, and these nodes in turn output the results of their calculations to other nodes. A simple example of a computational graph for the calculation a=(b+c)∗(c+2) can be seen below – we can break this calculation up into the following steps/nodes:\n",
        "\n",
        "dea=b+c=c+2=d∗e\n",
        "PyTorch tutorial - simple computational graph\n",
        "Simple computational graph\n",
        " \n",
        "\n",
        "The benefits of using a computational graph is that each node is like its own independently functioning piece of code (once it receives all its required inputs). This allows various performance optimizations to be performed in running the calculations such as threading and multiple processing / parallelism. All the major deep learning frameworks (TensorFlow, Theano, PyTorch etc.) involve constructing such computational graphs, through which neural network operations can be built and through which gradients can be back-propagated (if you’re unfamiliar with back-propagation, see my neural networks tutorial)."
      ]
    },
    {
      "metadata": {
        "id": "fwEPy4o3nP3J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "13881c90-7202-4cad-9d37-a6e01ffbf623"
      },
      "cell_type": "code",
      "source": [
        "!pip install torchvision\n",
        "!pip install torch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 3.6MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 8.4MB/s \n",
            "\u001b[?25hCollecting torch (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 22kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x587ac000 @  0x7fcdf7c7b2a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Installing collected packages: pillow, torch, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-0.4.1 torchvision-0.2.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1wz_XTk1nQz9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as pp\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import tensorflow.keras.datasets.mnist as mnist\n",
        "import tensorflow.keras as keras\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from __future__ import print_function, division\n",
        "from torch.autograd import Variable\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "60eibauvU9wK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "033f1bce-4bb0-485c-d7b4-efbf874489e5"
      },
      "cell_type": "code",
      "source": [
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mHello World !\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_NzYygxjRE-p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Tensors"
      ]
    },
    {
      "metadata": {
        "id": "FvTq1pwRybgx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "42b76fc0-d09f-4c25-f81e-9021ecfd9a12"
      },
      "cell_type": "code",
      "source": [
        "#Declare a tensor of 3D\n",
        "x=torch.Tensor(2,3,3)\n",
        "print(color.BOLD+ color.GREEN+\"torch.Tensor(2,3,3)\"+color.END)\n",
        "print(x)\n",
        "\n",
        "#Declare a tensor filled with random values\n",
        "y=torch.rand(2,3)\n",
        "print(color.BOLD+ color.GREEN+\"torch.rand(2,3)\"+color.END)\n",
        "print(y)\n",
        "\n",
        "#Add and Multiply Tensors\n",
        "z=torch.rand(2,3)\n",
        "print(color.BOLD+ color.RED+color.UNDERLINE+\"TENSOR SUM\"+color.END)\n",
        "print(color.BOLD+ color.BLUE+\"y\"+color.END)\n",
        "print(y)\n",
        "print(color.BOLD+ color.BLUE+\"z\"+color.END)\n",
        "print(z)\n",
        "print(color.BOLD+ color.BLUE+\"y+z\"+color.END)\n",
        "print(y+z)\n",
        "print(color.BOLD+ color.RED+color.UNDERLINE+\"TENSOR SCALAR PRODUCT\"+color.END)\n",
        "print(color.BOLD+ color.BLUE+\"y*z\"+color.END)\n",
        "print(y*z)\n",
        "z2=torch.rand(3,2)\n",
        "print(color.BOLD+ color.BLUE+\"z2\"+color.END)\n",
        "print(z2)\n",
        "print(color.BOLD+ color.RED+color.UNDERLINE+\"TENSOR MATRIX PRODUCT\"+color.END)\n",
        "print(color.BOLD+ color.GREEN+\"torch.matmul(y,z2)\"+color.END)\n",
        "print(torch.matmul(y,z2))\n",
        "a=torch.ones(2,3)\n",
        "\n",
        "print(color.BOLD+ color.BLUE+\"a\"+color.END)\n",
        "print(a)\n",
        "a[:,1] = a[:,1] + 1\n",
        "print(color.BOLD+\"We add 1 to column 2\"+color.END)\n",
        "print(color.BOLD+ color.GREEN+\"a[:,1] = a[:,1] + 1\"+color.END)\n",
        "print(color.BOLD+ color.BLUE+\"a after modification\"+color.END)\n",
        "print(a)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[92mtorch.Tensor(2,3,3)\u001b[0m\n",
            "tensor([[[1.8013e-35, 0.0000e+00, 1.3527e-35],\n",
            "         [0.0000e+00, 2.2200e-21, 4.5600e-41],\n",
            "         [1.3527e-35, 0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "        [[       nan, 1.4013e-45, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 5.0892e-14],\n",
            "         [1.4308e+34, 3.8178e-17, 4.5600e-41]]])\n",
            "\u001b[1m\u001b[92mtorch.rand(2,3)\u001b[0m\n",
            "tensor([[0.1198, 0.0400, 0.4098],\n",
            "        [0.8655, 0.2865, 0.4142]])\n",
            "\u001b[1m\u001b[91m\u001b[4mTENSOR SUM\u001b[0m\n",
            "\u001b[1m\u001b[94my\u001b[0m\n",
            "tensor([[0.1198, 0.0400, 0.4098],\n",
            "        [0.8655, 0.2865, 0.4142]])\n",
            "\u001b[1m\u001b[94mz\u001b[0m\n",
            "tensor([[0.1136, 0.7924, 0.2321],\n",
            "        [0.0056, 0.8588, 0.4267]])\n",
            "\u001b[1m\u001b[94my+z\u001b[0m\n",
            "tensor([[0.2334, 0.8324, 0.6419],\n",
            "        [0.8711, 1.1454, 0.8409]])\n",
            "\u001b[1m\u001b[91m\u001b[4mTENSOR SCALAR PRODUCT\u001b[0m\n",
            "\u001b[1m\u001b[94my*z\u001b[0m\n",
            "tensor([[0.0136, 0.0317, 0.0951],\n",
            "        [0.0049, 0.2461, 0.1767]])\n",
            "\u001b[1m\u001b[94mz2\u001b[0m\n",
            "tensor([[0.3233, 0.3359],\n",
            "        [0.6569, 0.0970],\n",
            "        [0.1171, 0.9628]])\n",
            "\u001b[1m\u001b[91m\u001b[4mTENSOR MATRIX PRODUCT\u001b[0m\n",
            "\u001b[1m\u001b[92mtorch.matmul(y,z2)\u001b[0m\n",
            "tensor([[0.1130, 0.4387],\n",
            "        [0.5165, 0.7173]])\n",
            "\u001b[1m\u001b[94ma\u001b[0m\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "\u001b[1mWe add 1 to column 2\u001b[0m\n",
            "\u001b[1m\u001b[92ma[:,1] = a[:,1] + 1\u001b[0m\n",
            "\u001b[1m\u001b[94ma after modification\u001b[0m\n",
            "tensor([[1., 2., 1.],\n",
            "        [1., 2., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TcBBCLKcdkgV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Autograd in PyTorch\n",
        "##Backpropagation"
      ]
    },
    {
      "metadata": {
        "id": "08lQMWlvPljv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "42f16d8d-f24b-4ca9-96fc-1387cc095340"
      },
      "cell_type": "code",
      "source": [
        "#We declare a variable with a tensor and specify it can backpropagates, meaning\n",
        "#it is trainable\n",
        "b = Variable(torch.ones(2, 2) * 2, requires_grad=True)\n",
        "\n",
        "c = 2 * (b  * b ) + 5 *b \n",
        "\n",
        "#Now we compute the gradient/derivatives of var2 with the backwards function\n",
        "#We store it in the grad attribute of b\n",
        "c.backward(torch.ones(2, 2))\n",
        "print(b.grad)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[13., 13.],\n",
            "        [13., 13.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RAzTKEyAhnbV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Creating a neural network in PyTorch\n",
        "##The neural network class"
      ]
    },
    {
      "metadata": {
        "id": "M0nOd7u_fQyd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#We use the nn class to declare a new class which inherits from it\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        #We specify the different layers of the Neural Nets\n",
        "        #First Layer--> Inputs : 28*28 connected to 200 cells\n",
        "        #Second Layer--> Neural layer of 200 cells connected to 200 cells\n",
        "        #Third Layer--> Neural layer of 200 cells connected to a 10-output layer\n",
        "        #Linear is for fully connected neural nets\n",
        "        self.fc1 = nn.Linear(28 * 28, 200)\n",
        "        self.fc2 = nn.Linear(200, 200)\n",
        "        self.fc3 = nn.Linear(200, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      #Forward specify how we move forward int the network\n",
        "      #How to go from a layer to another\n",
        "      #we use ReLu as an activation function\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return F.log_softmax(x)\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aqgEXPlGhpCy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "efb1310a-7293-4413-b0a0-7ba42d58d750"
      },
      "cell_type": "code",
      "source": [
        "#We declare an instance of this class\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
            "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
            "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YiB8O3_Rr4oO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Hyperparameters declaration"
      ]
    },
    {
      "metadata": {
        "id": "9t3XYCXHrmDf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate= 0.001\n",
        "epochs=10\n",
        "batch_size=200\n",
        "log_interval=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wXtCC4MQtzVp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Data Loading"
      ]
    },
    {
      "metadata": {
        "id": "IObEfDm2tvTi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose(\n",
        "                       [transforms.ToTensor(),transforms.Normalize((0.1307,),\n",
        "                                                                   (0.3081,))])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "      datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.1307,), (0.3081,))\n",
        "      ])),\n",
        "batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eG2WQyfjq0xp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Training the network\n",
        "###We need a loss function and an optimizer"
      ]
    },
    {
      "metadata": {
        "id": "Vqts9sq7quSL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a stochastic gradient descent optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "# create a loss function\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LvxfNrnOrCSL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5127
        },
        "outputId": "b796b0ff-faf3-4a76-fa0c-ad504fa81d6d"
      },
      "cell_type": "code",
      "source": [
        "# run the main training loop\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
        "        data = data.view(-1, 28*28)\n",
        "        optimizer.zero_grad()\n",
        "        net_out = net(data)\n",
        "        loss = criterion(net_out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                           100. * batch_idx / len(train_loader), loss.data[0]))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.325548\n",
            "Train Epoch: 0 [2000/60000 (3%)]\tLoss: 2.309134\n",
            "Train Epoch: 0 [4000/60000 (7%)]\tLoss: 2.267921\n",
            "Train Epoch: 0 [6000/60000 (10%)]\tLoss: 2.256472\n",
            "Train Epoch: 0 [8000/60000 (13%)]\tLoss: 2.221423\n",
            "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 2.200401\n",
            "Train Epoch: 0 [12000/60000 (20%)]\tLoss: 2.172950\n",
            "Train Epoch: 0 [14000/60000 (23%)]\tLoss: 2.131423\n",
            "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 2.096679\n",
            "Train Epoch: 0 [18000/60000 (30%)]\tLoss: 2.046123\n",
            "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 2.045399\n",
            "Train Epoch: 0 [22000/60000 (37%)]\tLoss: 1.957190\n",
            "Train Epoch: 0 [24000/60000 (40%)]\tLoss: 1.907361\n",
            "Train Epoch: 0 [26000/60000 (43%)]\tLoss: 1.792978\n",
            "Train Epoch: 0 [28000/60000 (47%)]\tLoss: 1.771238\n",
            "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 1.647434\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.557390\n",
            "Train Epoch: 0 [34000/60000 (57%)]\tLoss: 1.540496\n",
            "Train Epoch: 0 [36000/60000 (60%)]\tLoss: 1.381882\n",
            "Train Epoch: 0 [38000/60000 (63%)]\tLoss: 1.363821\n",
            "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 1.317532\n",
            "Train Epoch: 0 [42000/60000 (70%)]\tLoss: 1.167535\n",
            "Train Epoch: 0 [44000/60000 (73%)]\tLoss: 1.050259\n",
            "Train Epoch: 0 [46000/60000 (77%)]\tLoss: 1.077949\n",
            "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.873242\n",
            "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.911756\n",
            "Train Epoch: 0 [52000/60000 (87%)]\tLoss: 0.882716\n",
            "Train Epoch: 0 [54000/60000 (90%)]\tLoss: 0.874595\n",
            "Train Epoch: 0 [56000/60000 (93%)]\tLoss: 0.803681\n",
            "Train Epoch: 0 [58000/60000 (97%)]\tLoss: 0.731014\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.676517\n",
            "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.745367\n",
            "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.682577\n",
            "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.619037\n",
            "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.662266\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.650072\n",
            "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.597590\n",
            "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.597491\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.581686\n",
            "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.608809\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.480717\n",
            "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.515919\n",
            "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.518531\n",
            "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.384618\n",
            "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.481724\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.604505\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.456926\n",
            "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.555688\n",
            "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.533478\n",
            "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.518421\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.498745\n",
            "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.459142\n",
            "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.425932\n",
            "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.517141\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.395042\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.606072\n",
            "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.520808\n",
            "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.444427\n",
            "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.340016\n",
            "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.548024\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.425668\n",
            "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.395790\n",
            "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.363486\n",
            "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.499315\n",
            "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.417252\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.406255\n",
            "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.362279\n",
            "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.375372\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.422354\n",
            "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.374545\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.469064\n",
            "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.374278\n",
            "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.360591\n",
            "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.522761\n",
            "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.325833\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.394338\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.297091\n",
            "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.318820\n",
            "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.386974\n",
            "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.451445\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.378682\n",
            "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.327699\n",
            "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.330362\n",
            "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.201747\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.407579\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.328558\n",
            "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.376748\n",
            "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.332242\n",
            "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.348783\n",
            "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.374333\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.427912\n",
            "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 0.387949\n",
            "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.342174\n",
            "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.329241\n",
            "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.319476\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.370509\n",
            "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.288551\n",
            "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 0.493087\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.239454\n",
            "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.342304\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.375090\n",
            "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 0.338180\n",
            "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.357132\n",
            "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 0.250988\n",
            "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.277362\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.271768\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.383374\n",
            "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 0.302996\n",
            "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.333987\n",
            "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 0.285805\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.269317\n",
            "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.293518\n",
            "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.303181\n",
            "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 0.260779\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.433854\n",
            "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.279581\n",
            "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.191733\n",
            "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.256445\n",
            "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.270427\n",
            "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 0.289886\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.259711\n",
            "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 0.262054\n",
            "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.240802\n",
            "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.320695\n",
            "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.262616\n",
            "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.252494\n",
            "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.304835\n",
            "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 0.273621\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.244777\n",
            "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.282671\n",
            "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.374199\n",
            "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 0.252307\n",
            "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.387145\n",
            "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 0.399677\n",
            "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.331144\n",
            "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.259132\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.330568\n",
            "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 0.251221\n",
            "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.311417\n",
            "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 0.310391\n",
            "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.350440\n",
            "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.283466\n",
            "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.297482\n",
            "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 0.269722\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.396718\n",
            "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.268447\n",
            "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.238444\n",
            "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.321734\n",
            "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.297913\n",
            "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 0.262293\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.325599\n",
            "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 0.269386\n",
            "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.226157\n",
            "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.242346\n",
            "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.216219\n",
            "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.246057\n",
            "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.239196\n",
            "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 0.328907\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.236729\n",
            "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.331595\n",
            "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.229462\n",
            "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 0.197548\n",
            "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.201281\n",
            "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 0.354675\n",
            "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.230785\n",
            "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.285951\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.290524\n",
            "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 0.241449\n",
            "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.342156\n",
            "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 0.280872\n",
            "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.295750\n",
            "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.404803\n",
            "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.275973\n",
            "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 0.232152\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.242655\n",
            "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.242488\n",
            "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.301492\n",
            "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.268796\n",
            "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.312684\n",
            "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 0.281133\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.254407\n",
            "Train Epoch: 6 [2000/60000 (3%)]\tLoss: 0.301895\n",
            "Train Epoch: 6 [4000/60000 (7%)]\tLoss: 0.319884\n",
            "Train Epoch: 6 [6000/60000 (10%)]\tLoss: 0.254743\n",
            "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 0.291687\n",
            "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.342155\n",
            "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 0.284295\n",
            "Train Epoch: 6 [14000/60000 (23%)]\tLoss: 0.183558\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.297664\n",
            "Train Epoch: 6 [18000/60000 (30%)]\tLoss: 0.316745\n",
            "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.394602\n",
            "Train Epoch: 6 [22000/60000 (37%)]\tLoss: 0.226419\n",
            "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 0.206449\n",
            "Train Epoch: 6 [26000/60000 (43%)]\tLoss: 0.310517\n",
            "Train Epoch: 6 [28000/60000 (47%)]\tLoss: 0.278201\n",
            "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.200455\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.215139\n",
            "Train Epoch: 6 [34000/60000 (57%)]\tLoss: 0.184433\n",
            "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 0.230933\n",
            "Train Epoch: 6 [38000/60000 (63%)]\tLoss: 0.253430\n",
            "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.245926\n",
            "Train Epoch: 6 [42000/60000 (70%)]\tLoss: 0.252612\n",
            "Train Epoch: 6 [44000/60000 (73%)]\tLoss: 0.193978\n",
            "Train Epoch: 6 [46000/60000 (77%)]\tLoss: 0.171401\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.320471\n",
            "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.311639\n",
            "Train Epoch: 6 [52000/60000 (87%)]\tLoss: 0.264949\n",
            "Train Epoch: 6 [54000/60000 (90%)]\tLoss: 0.247444\n",
            "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 0.327392\n",
            "Train Epoch: 6 [58000/60000 (97%)]\tLoss: 0.211001\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.229882\n",
            "Train Epoch: 7 [2000/60000 (3%)]\tLoss: 0.242700\n",
            "Train Epoch: 7 [4000/60000 (7%)]\tLoss: 0.286480\n",
            "Train Epoch: 7 [6000/60000 (10%)]\tLoss: 0.257170\n",
            "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 0.240440\n",
            "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.273295\n",
            "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 0.159512\n",
            "Train Epoch: 7 [14000/60000 (23%)]\tLoss: 0.207475\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.251212\n",
            "Train Epoch: 7 [18000/60000 (30%)]\tLoss: 0.241624\n",
            "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.198086\n",
            "Train Epoch: 7 [22000/60000 (37%)]\tLoss: 0.231684\n",
            "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 0.298712\n",
            "Train Epoch: 7 [26000/60000 (43%)]\tLoss: 0.228578\n",
            "Train Epoch: 7 [28000/60000 (47%)]\tLoss: 0.265709\n",
            "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.349398\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.229419\n",
            "Train Epoch: 7 [34000/60000 (57%)]\tLoss: 0.180553\n",
            "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 0.205028\n",
            "Train Epoch: 7 [38000/60000 (63%)]\tLoss: 0.238868\n",
            "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.224226\n",
            "Train Epoch: 7 [42000/60000 (70%)]\tLoss: 0.302316\n",
            "Train Epoch: 7 [44000/60000 (73%)]\tLoss: 0.231504\n",
            "Train Epoch: 7 [46000/60000 (77%)]\tLoss: 0.254409\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.243102\n",
            "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.173231\n",
            "Train Epoch: 7 [52000/60000 (87%)]\tLoss: 0.276664\n",
            "Train Epoch: 7 [54000/60000 (90%)]\tLoss: 0.271549\n",
            "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 0.284719\n",
            "Train Epoch: 7 [58000/60000 (97%)]\tLoss: 0.205140\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.272582\n",
            "Train Epoch: 8 [2000/60000 (3%)]\tLoss: 0.234248\n",
            "Train Epoch: 8 [4000/60000 (7%)]\tLoss: 0.280198\n",
            "Train Epoch: 8 [6000/60000 (10%)]\tLoss: 0.214224\n",
            "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 0.309712\n",
            "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.205855\n",
            "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 0.262542\n",
            "Train Epoch: 8 [14000/60000 (23%)]\tLoss: 0.224429\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.282301\n",
            "Train Epoch: 8 [18000/60000 (30%)]\tLoss: 0.384786\n",
            "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.231612\n",
            "Train Epoch: 8 [22000/60000 (37%)]\tLoss: 0.276098\n",
            "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 0.187807\n",
            "Train Epoch: 8 [26000/60000 (43%)]\tLoss: 0.174043\n",
            "Train Epoch: 8 [28000/60000 (47%)]\tLoss: 0.170212\n",
            "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.287073\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.206800\n",
            "Train Epoch: 8 [34000/60000 (57%)]\tLoss: 0.265224\n",
            "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 0.221136\n",
            "Train Epoch: 8 [38000/60000 (63%)]\tLoss: 0.317135\n",
            "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.267534\n",
            "Train Epoch: 8 [42000/60000 (70%)]\tLoss: 0.229491\n",
            "Train Epoch: 8 [44000/60000 (73%)]\tLoss: 0.207780\n",
            "Train Epoch: 8 [46000/60000 (77%)]\tLoss: 0.191691\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.145785\n",
            "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.276942\n",
            "Train Epoch: 8 [52000/60000 (87%)]\tLoss: 0.231262\n",
            "Train Epoch: 8 [54000/60000 (90%)]\tLoss: 0.261037\n",
            "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 0.241446\n",
            "Train Epoch: 8 [58000/60000 (97%)]\tLoss: 0.219395\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.170502\n",
            "Train Epoch: 9 [2000/60000 (3%)]\tLoss: 0.179266\n",
            "Train Epoch: 9 [4000/60000 (7%)]\tLoss: 0.221149\n",
            "Train Epoch: 9 [6000/60000 (10%)]\tLoss: 0.217700\n",
            "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 0.198339\n",
            "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.271234\n",
            "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 0.325789\n",
            "Train Epoch: 9 [14000/60000 (23%)]\tLoss: 0.184369\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.279319\n",
            "Train Epoch: 9 [18000/60000 (30%)]\tLoss: 0.325427\n",
            "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.325075\n",
            "Train Epoch: 9 [22000/60000 (37%)]\tLoss: 0.240311\n",
            "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 0.356215\n",
            "Train Epoch: 9 [26000/60000 (43%)]\tLoss: 0.238739\n",
            "Train Epoch: 9 [28000/60000 (47%)]\tLoss: 0.265559\n",
            "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.258863\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.293710\n",
            "Train Epoch: 9 [34000/60000 (57%)]\tLoss: 0.295152\n",
            "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 0.295543\n",
            "Train Epoch: 9 [38000/60000 (63%)]\tLoss: 0.174385\n",
            "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.257483\n",
            "Train Epoch: 9 [42000/60000 (70%)]\tLoss: 0.305628\n",
            "Train Epoch: 9 [44000/60000 (73%)]\tLoss: 0.258031\n",
            "Train Epoch: 9 [46000/60000 (77%)]\tLoss: 0.212072\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.198192\n",
            "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.278524\n",
            "Train Epoch: 9 [52000/60000 (87%)]\tLoss: 0.255430\n",
            "Train Epoch: 9 [54000/60000 (90%)]\tLoss: 0.209430\n",
            "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 0.221852\n",
            "Train Epoch: 9 [58000/60000 (97%)]\tLoss: 0.188228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FeLMXDnIxarj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Testing the Model"
      ]
    },
    {
      "metadata": {
        "id": "UlYZSGLFsR8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "41474729-4c7e-4e80-d062-9f1a0a4bc5e1"
      },
      "cell_type": "code",
      "source": [
        "#We compile the loss of the test set\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "for data, target in test_loader:\n",
        "    data, target = Variable(data, volatile=True), Variable(target)\n",
        "    data = data.view(-1, 28 * 28)\n",
        "    net_out = net(data)\n",
        "    # sum up batch loss\n",
        "    test_loss += criterion(net_out, target).data[0]\n",
        "    pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.data).sum()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9369/10000 (93%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Id0vcagWt1-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "6d8dd56b-a230-4190-bb04-a357c78b424e"
      },
      "cell_type": "code",
      "source": [
        "data, target = Variable(data), Variable(target)\n",
        "data = data.view(-1, 28 * 28)\n",
        "pred = net_out.data.max(1)[1]  # get t\n",
        "\n",
        "pp.imshow(255-data, cmap='gray')\n",
        "pp.show()\n",
        "print(pred)\n",
        "\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAAjCAYAAACuA8JCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1cf65960b8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([2, 5, 1, 8, 3, 6, 5, 7, 9, 9, 7, 9, 3, 1, 4, 7, 2, 0, 2, 4, 7, 4, 0, 4,\n",
            "        9, 5, 4, 5, 8, 3, 9, 2, 6, 3, 9, 0, 8, 7, 0, 5, 9, 5, 1, 7, 3, 5, 6, 1,\n",
            "        5, 5, 1, 8, 1, 7, 6, 5, 7, 1, 0, 2, 1, 4, 8, 0, 6, 8, 1, 4, 5, 1, 0, 9,\n",
            "        7, 1, 4, 1, 5, 7, 5, 5, 4, 5, 4, 9, 9, 4, 7, 5, 8, 1, 8, 4, 9, 3, 1, 2,\n",
            "        5, 4, 4, 7, 6, 6, 5, 8, 2, 3, 2, 3, 2, 7, 8, 3, 5, 6, 0, 4, 2, 9, 6, 3,\n",
            "        6, 8, 8, 3, 8, 9, 4, 3, 3, 7, 7, 5, 9, 6, 3, 6, 7, 9, 7, 9, 9, 3, 9, 7,\n",
            "        1, 1, 0, 1, 2, 7, 2, 9, 9, 0, 1, 7, 0, 2, 2, 9, 5, 5, 1, 6, 8, 5, 9, 2,\n",
            "        7, 7, 2, 5, 5, 6, 1, 3, 1, 5, 3, 0, 2, 0, 4, 7, 0, 8, 9, 8, 0, 4, 2, 7,\n",
            "        0, 1, 9, 3, 8, 5, 0, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A3VPuXCwxdMk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}