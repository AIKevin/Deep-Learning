{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language Modeling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIKevin/Deep-Learning/blob/master/Language_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LA7TBZhOsNwt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wKsrbmQvsoEe",
        "colab_type": "code",
        "outputId": "3b5eb3e3-cd83-47b3-8594-a01d93cfee91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip\n",
        "!unzip -o data/ptb.zip -d data\n",
        "!cp data/ptb/reader.py .\n",
        "import reader"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data/ptb.zip\n",
            "   creating: data/ptb/\n",
            "  inflating: data/ptb/reader.py      \n",
            "   creating: data/__MACOSX/\n",
            "   creating: data/__MACOSX/ptb/\n",
            "  inflating: data/__MACOSX/ptb/._reader.py  \n",
            "  inflating: data/__MACOSX/._ptb     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hO7wTH3Qssg6",
        "colab_type": "code",
        "outputId": "6bc9881a-8b11-437a-e665-00ab9922a064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
        "!tar xzf simple-examples.tgz -C data/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-19 00:38:15--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
            "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
            "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34869662 (33M) [application/x-gtar]\n",
            "Saving to: ‘simple-examples.tgz’\n",
            "\n",
            "simple-examples.tgz   8%[>                   ]   2.85M   827KB/s    eta 42s    --2019-02-19 00:38:15--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
            "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
            "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34869662 (33M) [application/x-gtar]\n",
            "Saving to: ‘simple-examples.tgz’\n",
            "\n",
            "simple-examples.tgz   8%[>                   ]   2.85M   827KB/s    eta 42s    --2019-02-19 00:38:15--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
            "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
            "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34869662 (33M) [application/x-gtar]\n",
            "Saving to: ‘simple-examples.tgz’\n",
            "\n",
            "simple-examples.tgz 100%[===================>]  33.25M  1.66MB/s    in 22s     \n",
            "\n",
            "2019-02-19 00:38:38 (1.49 MB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
            "\n",
            "simple-examples.tgz 100%[===================>]  33.25M  1.66MB/s    in 22s     \n",
            "\n",
            "2019-02-19 00:38:38 (1.49 MB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
            "\n",
            "simple-examples.tgz 100%[===================>]  33.25M  1.66MB/s    in 22s     \n",
            "\n",
            "2019-02-19 00:38:38 (1.49 MB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o6gC90nD2k7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hpm={\n",
        "#Initial weight scale\n",
        "'init_scale' : 0.1,\n",
        "#Initial learning rate\n",
        "'learning_rate' : 1.0,\n",
        "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
        "'max_grad_norm' : 5,\n",
        "#The number of layers in our model\n",
        "'num_layers' : 2,\n",
        "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
        "'num_steps' : 20,\n",
        "#The number of processing units (neurons) in the hidden layers\n",
        "'hidden_size' : 256,\n",
        "#The maximum number of epochs trained with the initial learning rate\n",
        "'max_epoch' : 8,\n",
        "#The total number of epochs in training\n",
        "'max_max_epoch' : 8,\n",
        "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
        "#At 1, we ignore the Dropout Layer wrapping.\n",
        "'keep_prob' : 1,\n",
        "#The decay for the learning rate\n",
        "'decay' : 0.5,\n",
        "#The size for each batch of data\n",
        "'batch_size' : 30,\n",
        "#The size of our vocabulary\n",
        "'vocab_size' : 10000,\n",
        "#Training flag to separate training from testing\n",
        "'is_training' : 1,\n",
        "#Data directory for our dataset\n",
        "'data_dir' : \"data/simple-examples/data/\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nW9TI1YE2_80",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class PTBModel():\n",
        "\n",
        "    def __init__(self, is_training, hpm):\n",
        "      \n",
        "      self.hpm=hpm\n",
        "      #Placelholders\n",
        "      self._input_data = tf.placeholder(tf.int32, [self.hpm['batch_size'], self.hpm['num_steps']], name='input_data') #[30#20]\n",
        "      self._targets = tf.placeholder(tf.int32, [self.hpm['batch_size'], self.hpm['num_steps']], name='targets') #[30#20]\n",
        "      \n",
        "      #\n",
        "      lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.hpm['hidden_size'], forget_bias=0.0)\n",
        "      \n",
        "      \n",
        "      if is_training and self.hpm['keep_prob'] < 1:\n",
        "            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=self.hpm['keep_prob'])\n",
        "          \n",
        "      stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell] * self.hpm['num_layers'])\n",
        "      self._initial_state = stacked_lstm.zero_state(self.hpm['batch_size'], tf.float32)\n",
        "      with tf.device(\"/gpu:0\"):\n",
        "        embedding = tf.get_variable(\"embedding\", [hpm['vocab_size'], self.hpm['hidden_size']])  #[10000x200]\n",
        "        inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
        "        \n",
        "      if is_training and self.hpm['keep_prob'] < 1:\n",
        "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
        "          \n",
        "      outputs, state = tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=self._initial_state)\n",
        "      output_id= tf.identity(outputs, name='output')\n",
        "      output = tf.reshape(outputs, [-1, self.hpm['hidden_size']])\n",
        "      softmax_w = tf.get_variable(\"softmax_w\", [self.hpm['hidden_size'], self.hpm['vocab_size']]) #[200x1000]\n",
        "      softmax_b = tf.get_variable(\"softmax_b\", [self.hpm['vocab_size']]) #[1x1000]\n",
        "      logits = tf.matmul(output, softmax_w) + softmax_b\n",
        "      \n",
        "      logits=tf.identity(logits, name= 'output_fc')\n",
        "      \n",
        "      loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],\n",
        "                                                      [tf.ones([self.hpm['batch_size'] * self.hpm['num_steps']])])\n",
        "      self._cost = cost = tf.reduce_sum(loss) / self.hpm['batch_size']\n",
        "      self._final_state = state\n",
        "\n",
        "      #Everything after this point is relevant only for training\n",
        "      if not is_training:\n",
        "          return\n",
        "\n",
        "      self._lr = tf.Variable(0.0, trainable=False)\n",
        "      tvars = tf.trainable_variables()\n",
        "      grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), self.hpm['max_grad_norm'])\n",
        "      optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
        "      self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
        "      \n",
        "    def assign_lr(self, session, lr_value):\n",
        "      session.run(tf.assign(self._lr, lr_value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HGhQdJTYBuut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_data = reader.ptb_raw_data(hpm['data_dir'])\n",
        "train_data, valid_data, test_data, _, _ = raw_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SIIVWR0ZDN3F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_epoch(session, m, data, eval_op, verbose=False):\n",
        "\n",
        "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
        "    epoch_size = ((len(data) // m.hpm['batch_size']) - 1) // m.hpm['num_steps']\n",
        "    start_time = time.time()\n",
        "    costs = 0.0\n",
        "    iters = 0\n",
        "    #state = m.initial_state.eval()\n",
        "    #m.initial_state = tf.convert_to_tensor(m.initial_state) \n",
        "    #state = m.initial_state.eval()\n",
        "    state = session.run(m._initial_state)\n",
        "    \n",
        "    #For each step and data point\n",
        "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.hpm['batch_size'], m.hpm['num_steps'])):\n",
        " \n",
        "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
        "        cost, state, _ = session.run([m._cost, m._final_state, eval_op],\n",
        "                                     {m._input_data: x,\n",
        "                                      m._targets: y,\n",
        "                                      m._initial_state: state})\n",
        "        \n",
        "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
        "        costs += cost\n",
        "        \n",
        "        #Add number of steps to iteration counter\n",
        "        iters += m.hpm['num_steps']\n",
        "\n",
        "        if verbose and step % (epoch_size // 10) == 10:\n",
        "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" % (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
        "              iters * m.hpm['batch_size'] / (time.time() - start_time)))\n",
        "\n",
        "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
        "    return np.exp(costs / iters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JoIQpxBGB1Hg",
        "colab_type": "code",
        "outputId": "d54ebc7a-4e6c-4e30-bc6b-df6d00a745fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-s8G4zE6FBbw",
        "colab_type": "code",
        "outputId": "21dc33b8-d25b-4963-e018-7b9dbf30b67a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1781
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Initializes the Execution Graph and the Session\n",
        "with tf.Graph().as_default(), tf.Session() as session:\n",
        "    initializer = tf.random_uniform_initializer(-hpm['init_scale'],hpm['init_scale'])\n",
        "    \n",
        "    # Instantiates the model for training\n",
        "    # tf.variable_scope add a prefix to the variables created with tf.get_variable\n",
        "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
        "        m = PTBModel(True, hpm)\n",
        "    saver= tf.train.Saver()\n",
        "    # Reuses the trained parameters for the validation and testing models\n",
        "    # They are different instances but use the same variables for weights and biases, they just don't change when data is input\n",
        "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
        "        mvalid = PTBModel(False, hpm)\n",
        "        mtest = PTBModel(False, hpm)\n",
        "\n",
        "    #Initialize all variables\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    for i in range(m.hpm['max_max_epoch']):\n",
        "        # Define the decay for this epoch\n",
        "        lr_decay = m.hpm['decay'] ** max(i - m.hpm['max_epoch'], 0.0)\n",
        "        \n",
        "        # Set the decayed learning rate as the learning rate for this epoch\n",
        "        m.assign_lr(session, m.hpm['learning_rate'] * lr_decay)\n",
        "\n",
        "        print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, session.run(m._lr)))\n",
        "        \n",
        "        # Run the loop for this epoch in the training model\n",
        "        train_perplexity = run_epoch(session, m, train_data, m._train_op,\n",
        "                                   verbose=True)\n",
        "        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
        "        \n",
        "        # Run the loop for this epoch in the validation model\n",
        "        valid_perplexity = run_epoch(session, mvalid, valid_data, tf.no_op())\n",
        "        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
        "    \n",
        "    # Run the loop in the testing model to see how effective was our training\n",
        "    test_perplexity = run_epoch(session, mtest, test_data, tf.no_op())\n",
        "    \n",
        "    print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
        "    save_path= saver.save(session, \"/gdrive/My Drive/Colab Notebooks/Checkpoints/LMmodel.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Learning rate: 1.000\n",
            "0.006 perplexity: 7494.461 speed: 10055 wps\n",
            "0.106 perplexity: 1105.156 speed: 14202 wps\n",
            "0.205 perplexity: 812.629 speed: 14301 wps\n",
            "0.305 perplexity: 647.097 speed: 14356 wps\n",
            "0.404 perplexity: 534.577 speed: 14385 wps\n",
            "0.504 perplexity: 468.034 speed: 14408 wps\n",
            "0.603 perplexity: 419.276 speed: 14411 wps\n",
            "0.702 perplexity: 383.488 speed: 14417 wps\n",
            "0.802 perplexity: 356.537 speed: 14417 wps\n",
            "0.901 perplexity: 331.548 speed: 14417 wps\n",
            "Epoch 1 : Train Perplexity: 312.051\n",
            "Epoch 1 : Valid Perplexity: 186.312\n",
            "Epoch 2 : Learning rate: 1.000\n",
            "0.006 perplexity: 206.086 speed: 11974 wps\n",
            "0.106 perplexity: 182.886 speed: 14178 wps\n",
            "0.205 perplexity: 175.493 speed: 14266 wps\n",
            "0.305 perplexity: 167.841 speed: 14317 wps\n",
            "0.404 perplexity: 159.845 speed: 14334 wps\n",
            "0.504 perplexity: 156.608 speed: 14350 wps\n",
            "0.603 perplexity: 153.043 speed: 14364 wps\n",
            "0.702 perplexity: 149.914 speed: 14371 wps\n",
            "0.802 perplexity: 147.705 speed: 14383 wps\n",
            "0.901 perplexity: 144.022 speed: 14391 wps\n",
            "Epoch 2 : Train Perplexity: 141.531\n",
            "Epoch 2 : Valid Perplexity: 147.894\n",
            "Epoch 3 : Learning rate: 1.000\n",
            "0.006 perplexity: 138.711 speed: 12140 wps\n",
            "0.106 perplexity: 126.051 speed: 14280 wps\n",
            "0.205 perplexity: 123.528 speed: 14370 wps\n",
            "0.305 perplexity: 119.392 speed: 14366 wps\n",
            "0.404 perplexity: 114.959 speed: 14372 wps\n",
            "0.504 perplexity: 114.058 speed: 14374 wps\n",
            "0.603 perplexity: 112.511 speed: 14389 wps\n",
            "0.702 perplexity: 111.017 speed: 14404 wps\n",
            "0.802 perplexity: 110.137 speed: 14413 wps\n",
            "0.901 perplexity: 108.065 speed: 14414 wps\n",
            "Epoch 3 : Train Perplexity: 106.877\n",
            "Epoch 3 : Valid Perplexity: 135.208\n",
            "Epoch 4 : Learning rate: 1.000\n",
            "0.006 perplexity: 110.886 speed: 12300 wps\n",
            "0.106 perplexity: 101.603 speed: 14269 wps\n",
            "0.205 perplexity: 100.103 speed: 14416 wps\n",
            "0.305 perplexity: 96.984 speed: 14454 wps\n",
            "0.404 perplexity: 93.579 speed: 14474 wps\n",
            "0.504 perplexity: 93.149 speed: 14469 wps\n",
            "0.603 perplexity: 92.225 speed: 14462 wps\n",
            "0.702 perplexity: 91.292 speed: 14465 wps\n",
            "0.802 perplexity: 90.838 speed: 14466 wps\n",
            "0.901 perplexity: 89.415 speed: 14462 wps\n",
            "Epoch 4 : Train Perplexity: 88.705\n",
            "Epoch 4 : Valid Perplexity: 130.050\n",
            "Epoch 5 : Learning rate: 1.000\n",
            "0.006 perplexity: 92.952 speed: 12550 wps\n",
            "0.106 perplexity: 87.189 speed: 14311 wps\n",
            "0.205 perplexity: 85.813 speed: 14367 wps\n",
            "0.305 perplexity: 83.336 speed: 14408 wps\n",
            "0.404 perplexity: 80.590 speed: 14422 wps\n",
            "0.504 perplexity: 80.495 speed: 14415 wps\n",
            "0.603 perplexity: 79.842 speed: 14420 wps\n",
            "0.702 perplexity: 79.269 speed: 14441 wps\n",
            "0.802 perplexity: 79.056 speed: 14446 wps\n",
            "0.901 perplexity: 77.945 speed: 14450 wps\n",
            "Epoch 5 : Train Perplexity: 77.513\n",
            "Epoch 5 : Valid Perplexity: 128.946\n",
            "Epoch 6 : Learning rate: 1.000\n",
            "0.006 perplexity: 83.800 speed: 11988 wps\n",
            "0.106 perplexity: 77.965 speed: 14239 wps\n",
            "0.205 perplexity: 76.847 speed: 14317 wps\n",
            "0.305 perplexity: 74.620 speed: 14319 wps\n",
            "0.404 perplexity: 72.255 speed: 14356 wps\n",
            "0.504 perplexity: 72.305 speed: 14377 wps\n",
            "0.603 perplexity: 71.858 speed: 14387 wps\n",
            "0.702 perplexity: 71.451 speed: 14393 wps\n",
            "0.802 perplexity: 71.280 speed: 14392 wps\n",
            "0.901 perplexity: 70.352 speed: 14403 wps\n",
            "Epoch 6 : Train Perplexity: 70.042\n",
            "Epoch 6 : Valid Perplexity: 129.597\n",
            "Epoch 7 : Learning rate: 1.000\n",
            "0.006 perplexity: 76.466 speed: 12285 wps\n",
            "0.106 perplexity: 71.519 speed: 14252 wps\n",
            "0.205 perplexity: 70.365 speed: 14349 wps\n",
            "0.305 perplexity: 68.548 speed: 14363 wps\n",
            "0.404 perplexity: 66.515 speed: 14357 wps\n",
            "0.504 perplexity: 66.651 speed: 14370 wps\n",
            "0.603 perplexity: 66.323 speed: 14372 wps\n",
            "0.702 perplexity: 66.027 speed: 14375 wps\n",
            "0.802 perplexity: 65.959 speed: 14386 wps\n",
            "0.901 perplexity: 65.159 speed: 14388 wps\n",
            "Epoch 7 : Train Perplexity: 64.947\n",
            "Epoch 7 : Valid Perplexity: 130.090\n",
            "Epoch 8 : Learning rate: 1.000\n",
            "0.006 perplexity: 70.337 speed: 12489 wps\n",
            "0.106 perplexity: 67.021 speed: 14319 wps\n",
            "0.205 perplexity: 65.917 speed: 14447 wps\n",
            "0.305 perplexity: 64.213 speed: 14428 wps\n",
            "0.404 perplexity: 62.321 speed: 14445 wps\n",
            "0.504 perplexity: 62.452 speed: 14439 wps\n",
            "0.603 perplexity: 62.150 speed: 14433 wps\n",
            "0.702 perplexity: 61.939 speed: 14435 wps\n",
            "0.802 perplexity: 61.906 speed: 14436 wps\n",
            "0.901 perplexity: 61.201 speed: 14432 wps\n",
            "Epoch 8 : Train Perplexity: 61.063\n",
            "Epoch 8 : Valid Perplexity: 132.350\n",
            "Test Perplexity: 126.495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zSszCdw7Mr66",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}